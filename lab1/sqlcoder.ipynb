{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48d001cd-ab42-4d18-bb88-ce1495184861",
   "metadata": {},
   "source": [
    "# Deploy a Huggingface LLM on SageMaker\n",
    "## Overview\n",
    "Amazon SageMaker provides a fully managed model hosting capability for any machine learning (ML) models for inferences. Specifically, SageMaker hosting offers a broad selection of ML infrastructure and model deployment options to help meet all your ML inference needs. \n",
    "\n",
    "## Deploy SqlCoder LLM on SageMaker\n",
    "SQLCoder is a family of state-of-the-art LLMs for converting natural language questions to SQL queries. [SQLCoder](https://github.com/defog-ai/sqlcoder) has shown impressive benchmark that outperforms GPT4 and GPT4-Turbo on text to SQL generation task. In this lab, we'll deploy a quantized version of this model to a SageMaker endpoint using SageMaker LMI, running inference on it and validate the inference results. To optimize the deployment and inference, we'll use SageMaker LMI to host the model in a SageMaker endpoint.\n",
    "\n",
    "## SageMaker LMI Containers\n",
    "SageMaker LMI containers are a set of high performance Docker Containers purpose built for large language model (LLM) inference. With these containers you can leverage high performance open-source inference libraries like vLLM, TensorRT-LLM, DeepSpeed, Transformers NeuronX to deploy LLMs on AWS SageMaker Endpoints. These containers bundle together a model server with open-source inference libraries to deliver an all-in-one LLM serving solution. We provide quick start notebooks that get you deploying popular open source models in minutes, and advanced guides to maximize performance of your endpoint.\n",
    "\n",
    "The lab can be organized into the following key steps:\n",
    "1. Create a model service configuration file that specifies a 4-Bit quantized SQLCoder available on Huggingface Hub.\n",
    "2. Deploy the model to SageMaker as a realtime endpoint using SageMaker LMI.\n",
    "3. Test and verify the model is deployed successfully and able to send inference requests to the model for SQL query generation.\n",
    "4. Setup a test database called **Chinook** running as SQLite, an in memory database.\n",
    "5. Sends requests to the SQLCoder LLM using natural language as the LLM prompt.\n",
    "6. Invokes the model and receives a response query.\n",
    "7. Use the response query to invoke against the test SQLite database and verify the results.\n",
    "\n",
    "\n",
    "\n",
    "> This notebook has been tested in a **`SageMaker Distribution 1.4`** Image using **Base Python 3.0 kernel** on  **ml.m5.large** instance.\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3975bfbd-9b06-4207-9244-6478c2ddeb40",
   "metadata": {},
   "source": [
    "First, let's install all the required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b7f5d5-bc86-4bf6-9429-b7c220f6ad8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install boto3 sagemaker fmeval jsonlines transformers -Uq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4dbc39-b906-40b4-b626-59c32a30e12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -Iv pandas==2.1.4 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a64ed4-7886-4e12-8433-8e3b4440beca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92ac487-6a30-4c83-9248-b9b4080767e3",
   "metadata": {},
   "source": [
    "Import the library to be used throughout the lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaaa4d6-b004-4d71-a802-fad9928d86a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import Model, image_uris, serializers, deserializers\n",
    "import json\n",
    "import sqlite3\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "account_id = sess.account_id()  # account_id of the current SageMaker Studio environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a525423-1a67-41a2-9a29-913b44b0ee05",
   "metadata": {},
   "source": [
    "# Deploy a Huggingface LLM Using SageMaker LMI\n",
    "In order to deploy an LLM using SageMAker LMI, we need to setup a configuration file with key information about how the model should be hosted and serving inferences. For instance, to deploy a huggingface LLM, we only need to provide the huggingface model ID in the configuration, SageMaker LMI will automatically take care of downloading the model and loads the model in the serving container. SgaeMaker LMI is highly configurable to provide users the flexibility in choosing the most optmized configuration to serve their models. Please refer to this [link](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-configuration.html) to lear more about the the avaialble configuration parameters for LMI. \n",
    "\n",
    "The following diagram gives an overview of a SageMaker LMI deployment pipeline you can use to deploy your models.\n",
    "\n",
    "![SageMaker LMI Deployment](images/sm_lmi_pipeline.jpg)\n",
    "\n",
    "This [blog post](https://aws.amazon.com/blogs/machine-learning/boost-inference-performance-for-llms-with-new-amazon-sagemaker-containers/) provides great amount of detail about SageMaker LMI, the infernce optimization frameworks that it supports and the performance benchmarks for each of the supported frameworks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b78b612-749e-4eab-93bb-17b31ce833d0",
   "metadata": {},
   "source": [
    "First, we provide a serving.properties file with the model specific details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25bd807-45bb-4482-8c67-4acade890b61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile serving.properties\n",
    "engine=Python\n",
    "option.model_id=TheBloke/sqlcoder-34b-alpha-GPTQ\n",
    "option.task=text-generation\n",
    "option.trust_remote_code=true\n",
    "option.tensor_parallel_degree=max\n",
    "option.rolling_batch=vllm\n",
    "option.quantize=gptq\n",
    "option.rolling_batch=auto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65d2917-9a34-46e2-bd9f-8442dd5dbf43",
   "metadata": {},
   "source": [
    "In the following, we'll create a tar file with only the service.properties file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c12d6f8-ebc4-4ebb-84d2-bdefe07c4d9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "mkdir model\n",
    "mv serving.properties model/\n",
    "tar czvf sqlcoder.tar.gz model/\n",
    "rm -rf model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b57672-ed9a-44dc-a8ff-705955d71c7d",
   "metadata": {},
   "source": [
    "Define an LMI container to use by specifying the framework and the framework version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d380ac71-0425-4583-a5ec-ce01ff2bc262",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_uri = image_uris.retrieve(\n",
    "        framework=\"djl-deepspeed\",\n",
    "        region=sess.boto_session.region_name,\n",
    "        version=\"0.26.0\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab697e40-a053-4999-b75c-d7c72fa5e0a7",
   "metadata": {},
   "source": [
    "Uploads the .gz file to S3 for serving container to pick up at model deployment time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b99630f-d660-42aa-912f-27ee97946160",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_code_prefix = \"models/large-model-lmi/sqlcoder\"\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "code_artifact = sess.upload_data(\"sqlcoder.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- &gt; {code_artifact}\")\n",
    "\n",
    "model = Model(image_uri=image_uri, model_data=code_artifact, role=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcced03-fac4-4cb8-a3ac-735d4021eb9c",
   "metadata": {},
   "source": [
    "Specifies the mode and use SageMaker SDK to trigger the model deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192747f1-944b-4970-9155-f9a528e6c91e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instance_type = \"ml.g5.2xlarge\"\n",
    "endpoint_name = sagemaker.utils.name_from_base(\"sqlcoder-lmi-model\")\n",
    "\n",
    "model.deploy(initial_instance_count=1,\n",
    "             instance_type=instance_type,\n",
    "             endpoint_name=endpoint_name,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8b1efc-98a5-4896-8126-951d3e2e039d",
   "metadata": {},
   "source": [
    "Once the model is deployed successfully, we can start running inference against the endpoint. SageMaker SDK provides a Predictor class that helps simplifying inference request and response. In the following cell, we'll create a predictor object for the endpoint so that we could use it for generating SQL query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f98b34f-ecb1-4cdc-b132-790db5454570",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# our requests and responses will be in json format so we specify the serializer and the deserializer\n",
    "predictor = sagemaker.Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sess,\n",
    "    serializer=serializers.JSONSerializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d975f9-28f6-4431-b3cc-d9ea8202285b",
   "metadata": {},
   "source": [
    "Here's a sample prompt template that consists of an instruction with placeholders to be sent to SQLCoder LLM for inference. You can modify the template and observe how different prompts would impact the response from the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786a0921-86d1-4751-b564-944ba8451c90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"### Task\n",
    "Generate a SQL query to answer [QUESTION]{user_question}[/QUESTION]\n",
    "\n",
    "### Instructions\n",
    "- If you cannot answer the question with the available database schema, return 'I do not know'\n",
    "\n",
    "### Database Schema\n",
    "The query will run on a database with the following schema:\n",
    "{table_metadata_string}\n",
    "\n",
    "### Answer\n",
    "Given the database schema, here is the SQL query that answers [QUESTION]{user_question}[/QUESTION]\n",
    "[SQL]\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13445662-bb02-461a-86d2-0e837785510f",
   "metadata": {},
   "source": [
    "## SQL Query Validation\n",
    "At this point, we will start validating the LLM capability by sending the generated SQL query to a test database. For this example, We use [Chinook](https://github.com/lerocha/chinook-database) database which contains sample Music album sales across music companies. We also created `metadata.sql` that contains the DDL for the database schema. The schema information is to be fed to the prompt template above to complete a prompt. \n",
    "\n",
    "The following database diagram illustrates the chinook database tables and their relationships.\n",
    "\n",
    "![chinook schema](images/chinook-schema.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57338c51-b096-43bb-a90c-2ac36773bb3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metadata_file = \"metadata.sql\"\n",
    "with open(metadata_file, \"r\") as f:\n",
    "    table_metadata_string = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed468f8e-6b09-4e91-95b8-f52ab35afcc1",
   "metadata": {},
   "source": [
    "Let's ask a question using natural language relevant to the given DB schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0e094c-dc9f-4379-9b09-7112a1b3598c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"how many unique albums are there?\"\n",
    "prompt = prompt_template.format(user_question=question, table_metadata_string=table_metadata_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f497162c-3439-4c1c-acc6-a58d46017577",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af59bd41-5d41-4fc8-aa8d-da0fcbe783db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def invoke_model(prompt):\n",
    "    response = predictor.predict(\n",
    "        {\"inputs\": prompt, \"parameters\": {\"max_tokens\":1024}}\n",
    "    )\n",
    "    output = response.decode(\"utf-8\")\n",
    "    full_output_text = json.loads(output)[\"generated_text\"]\n",
    "    sql_query = full_output_text.split(\"[/SQL]\")[0]\n",
    "    return sql_query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e885c206-d22e-42db-a4bb-3dea50f5c106",
   "metadata": {},
   "source": [
    "Invokes the LLM for SQL generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695cce3f-7e0a-4245-b493-f25a39306577",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sql_query = invoke_model(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e59a84a-4e62-4aa5-a758-eef2242fed00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(sql_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ef24e7-4660-42df-b9c8-7cc613304992",
   "metadata": {},
   "source": [
    "Next, we'll start creating a database connection to a test db hosted in memory. \n",
    "We'll also download the dataset from the given link so that they could be populated into the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6208d128-75ba-47e0-8df1-e5169a12c139",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "connection = sqlite3.connect(\"test.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3706aba-08b7-477a-af7c-24789d726a20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db_file_url = \"https://github.com/lerocha/chinook-database/releases/download/v1.4.5/Chinook_Sqlite.sqlite\"\n",
    "db_filename = \"Chinook_Sqlite.sqlite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78c8d89-83d3-4039-aa9c-164c17950006",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5723b0e6-9808-4f4a-951d-7dd3ff85532e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "urlretrieve(db_file_url, db_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cf31d6-cf24-465d-b97b-7ba58b74314f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_query(query):\n",
    "    # Create a SQL connection to our SQLite database\n",
    "    con = sqlite3.connect(db_filename)\n",
    "    cur = con.cursor()\n",
    "\n",
    "    # The result of a \"cursor.execute\" can be iterated over by row\n",
    "    for row in cur.execute(query):\n",
    "        print(row)\n",
    "    # Be sure to close the connection\n",
    "    con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317d496f-8415-47f1-8d35-3fee9e49f689",
   "metadata": {},
   "source": [
    "## Use SageMaker Foundation Model Evaluation (fmeval) to evaluate SQLCoder\n",
    "### Foundation Model Evaluations Library\n",
    "fmeval is an open source library to evaluate Large Language Models (LLMs) in order to help select the best LLM for your use case. The library evaluates LLMs for the following tasks:\n",
    "\n",
    "* Open-ended generation - The production of natural human responses to text that does not have a pre-defined structure.\n",
    "* Text summarization - The generation of a condensed summary retaining the key information contained in a longer text.\n",
    "* Question Answering - The generation of a relevant and accurate response to an answer.\n",
    "* Classification - Assigning a category, such as a label or score to text, based on its content.\n",
    "\n",
    "To learn more about how to use `fmeval` library, please follow this [github](https://github.com/aws/fmeval) repository and [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-foundation-model-evaluate-overview.html).\n",
    "\n",
    "For SQL generation task, we'll leverage the Q&A evaluation task to measure the accuracy of the generated query.\n",
    "For this lab, we've curated a few Q&A smaples to serve as the ground truth data. The `fmeval` will use these data to perform evaluation on the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26296f72-b84e-4a3d-ba05-4dccc26630b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "\n",
    "input_file = \"fmeval_data_inputs.jsonl\"\n",
    "output_file = \"fmeval_data_outputs.jsonl\"\n",
    "\n",
    "# For each line in `input_file`, invoke the model using the input from that line,\n",
    "# augment the line with the invocation results, and write the augmented line to `output_file`.\n",
    "with jsonlines.open(input_file) as input_fh, jsonlines.open(output_file, \"w\") as output_fh:\n",
    "    for line in input_fh:\n",
    "        if \"question\" in line:\n",
    "            question = line[\"question\"]\n",
    "            print(f\"Question: {question}\")\n",
    "            p = prompt_template.format(user_question=question, table_metadata_string=table_metadata_string)\n",
    "            output = invoke_model(p)\n",
    "            print(f\"Model output: {output}\")\n",
    "            print(\"==============================\")\n",
    "            line[\"model_output\"] = output\n",
    "            output_fh.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0450ba7-dc9a-473f-8a65-5751f4b19457",
   "metadata": {},
   "source": [
    "## FMEval Setup\n",
    "\n",
    "In this section, we will perform the evaluation on the model that we deployed. We will a ModelRunner to evaluate the model on Accuracy using the FMEval library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71c38a2-ac07-426f-a888-9acaf88d3a62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fmeval.data_loaders.data_config import DataConfig\n",
    "from fmeval.constants import MIME_TYPE_JSONLINES\n",
    "from fmeval.eval_algorithms.qa_accuracy import QAAccuracy, QAAccuracyConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b55711-2398-44a7-829c-8630ef31e412",
   "metadata": {},
   "source": [
    "### Data Config Setup\n",
    "Below, we create a DataConfig for the local dataset file we just created, trex_sample_with_model_outputs.jsonl.\n",
    "\n",
    "* dataset_name is just an identifier for your own reference\n",
    "* dataset_uri is either a local path to a file or an S3 URI\n",
    "* dataset_mime_type is the MIME type of the dataset. Currently, JSON and JSON Lines are supported.\n",
    "* model_input_location, target_output_location, and model_output_location are JMESPath queries used to find the model inputs, target outputs, and model outputs within the dataset. The values that you specify here depend on the structure of the dataset itself. Take a look at trex_sample_with_model_outputs.jsonl to see where \"question\", \"answers\", and \"model_output\" show up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377f0764-7e5f-4d97-82ff-32c508c17df1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = DataConfig(\n",
    "    dataset_name=\"llm_sample_with_model_outputs\",\n",
    "    dataset_uri=\"fmeval_data_outputs.jsonl\",\n",
    "    dataset_mime_type=MIME_TYPE_JSONLINES,\n",
    "    model_input_location=\"question\",\n",
    "    target_output_location=\"answers\",\n",
    "    model_output_location=\"model_output\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025dee41-809b-426c-a12e-a1a20a9b235c",
   "metadata": {},
   "source": [
    "## Run Evaluation\n",
    "In use cases that we demonstrate in the other example notebooks, we usually pass a model runner and prompt template to the evaluate method of our evaluation algorithm. However, since our dataset already contains all of the model inference outputs, we only need to pass our dataset config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8a0938-94e6-4fe2-bffd-b3d049fd620a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_algo = QAAccuracy(QAAccuracyConfig(target_output_delimiter=\"<OR>\"))\n",
    "eval_output = eval_algo.evaluate(dataset_config=config, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0750d7f4-c8fe-48d0-ac64-5d8418c5f9b5",
   "metadata": {},
   "source": [
    "### Parse Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f6b4f4-467e-427c-8a73-eafb535b3f47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pretty-print the evaluation output (notice the score).\n",
    "import json\n",
    "print(json.dumps(eval_output, default=vars, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67a295e-972d-4e76-9a4b-c664cbc26122",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_output[0].dataset_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3385073e-0a61-497e-84bb-ac1854241cbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a Pandas DataFrame to visualize the results\n",
    "import pandas as pd\n",
    "\n",
    "data = []\n",
    "\n",
    "# We obtain the path to the results file from \"output_path\" in the cell above\n",
    "with open(\"fmeval_data_outputs.jsonl\", \"r\") as file:\n",
    "    for line in file:\n",
    "        data_dict = json.loads(line)\n",
    "        data_dict[\"eval_f1_score\"] = eval_output[0].dataset_scores[0].value\n",
    "        data_dict[\"eval_exact_match_score\"] = eval_output[0].dataset_scores[1].value\n",
    "        data_dict[\"eval_quasi_exact_match_score\"] = eval_output[0].dataset_scores[2].value\n",
    "        data_dict[\"eval_quasi_precision_over_words_score\"] = eval_output[0].dataset_scores[3].value\n",
    "        data_dict[\"eval_quasi_recall_over_words_score\"] = eval_output[0].dataset_scores[4].value\n",
    "        data.append(data_dict)\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef015fa-6100-4edd-bd50-a3d3e426bad8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7168e3-7c90-4792-a550-206703e22188",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "In this lab, we learn how to deploy a Huggingface model (SQLCoder) in a SageMaker Deep Learning Container using SageMaker SDK. \n",
    "\n",
    "To test the deployed LLM, we used a natural language to ask a question and have the LLM to generate a relevant SQL query based on the given context. \n",
    "\n",
    "We also loaded a test database using SQLLite with sample data so that we could use the generated query against the database to fetch the results. \n",
    "\n",
    "Additionally, we used an open source FM evaluation framework called FMEval to evaluate the performance of the model. \n",
    "\n",
    "Finally, we showed the evaluation results as a pandas dataframe for visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ed5d4a-fffb-4e0f-9a76-72496b603cfe",
   "metadata": {},
   "source": [
    "# Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278f088f-a9d3-4274-ae1c-29850876353a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.delete_endpoint(endpoint_name)\n",
    "sess.delete_endpoint_config(endpoint_name)\n",
    "model.delete_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
