{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "719cd242-fbb5-44b4-988c-119ca0fbe1a6",
   "metadata": {},
   "source": [
    "# SageMaker Serial Inference Pipeline \n",
    "## Introduction\n",
    "An **inference pipeline** is a Amazon SageMaker model that is composed of a linear sequence of 2 to 15 containers that process requests for inferences on data. You use an inference pipeline to define and deploy any combination of pretrained SageMaker built-in algorithms and your own custom algorithms packaged in Docker containers. You can use an inference pipeline to combine **preprocessing**, **predictions**, and **post-processing** data science tasks. Inference pipelines are fully managed.\n",
    "\n",
    "Here's an architecture diagram that depicts a standard serial inference pipeline in SageMaker:\n",
    "\n",
    "![sm inference pipeline](images/sm-serial-inference-pipeline.jpg) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda3be00-a385-4109-84ad-40d18b6163fa",
   "metadata": {},
   "source": [
    "## Solution Overview\n",
    "The focus of this notebook is to build an ML solution using SageMaker serial inference pipeline. \n",
    "\n",
    "To demonstrate the capability, we chose a recommendation use case that uses Neural Collaboritive Filtering (NCF) based on a project [here](https://github.com/aws-samples/amazon-sagemaker-custom-recommender-system). This prooject is also available in SageMaker Jumpstart. (In SageMaker Jumpstart, search for **Customized Recommender System**).\n",
    "\n",
    "Additionally, we'll make some modification on the original model architecture to make a serial inference pipeline, as depicted in the following architecture:\n",
    "\n",
    "![serial_inference](images/serial-inference-pipeline.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6907e13-2304-44ad-8a12-329ee67c9d66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.model import SKLearnModel\n",
    "from sagemaker.tensorflow.model import TensorFlowModel\n",
    "from sagemaker.local import LocalSession\n",
    "from sagemaker import Session\n",
    "import boto3\n",
    "import sagemaker\n",
    "from PIL import Image\n",
    "import io\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabc427b-1da4-4466-a8c3-0ddcc61c37b0",
   "metadata": {},
   "source": [
    "# Load the models\n",
    "As shown in the previous architecture diagram, the serial inference pipeline is made of 2 ML models:\n",
    "\n",
    "1. A KNN model that finds topk similar movies based on the given input\n",
    "2. A tensorflow model that ranks the movies based on the predicted relevance scores.\n",
    "\n",
    "For the first model, we've provided a pretrained KNN model in this lab so that you dont' need to go through the training process. The KNN model was trained on the following dataset:\n",
    "\n",
    "* **Data source**: Movielens(ml-latest-small) (http://movielens.org)\n",
    "\n",
    "In addition, the model is used in the serial inference pipeline to fetch the topk movies similar to the input title. \n",
    "\n",
    "For the second model, we leverage a trained tensorflow recommender model to perform rankings of the movies from the KNN model. The model weights can be found in the given S3 bucket so that it could be deployed into SageMaker environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb182164-fc5a-4727-9812-710d03fa3964",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "cd knn_model/pkl \n",
    "tar -czvf ../knnmodel.tar.gz ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a9f6ba-c30c-487c-969b-e3a9be1b5538",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_code_prefix = \"models/knn\"\n",
    "bucket = session.default_bucket()  # bucket to house the model artifacts\n",
    "knn_model_artifact = session.upload_data(\"knn_model/knnmodel.tar.gz\", bucket, s3_code_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4c5f47-edec-4c34-9d9a-7ab5d5deac13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "s3_model_artifact_tensorflow = \"s3://sagemaker-solutions-prod-us-east-1/0.2.0/Customized-recommender-system/1.0.0/artifacts/model/model.tar.gz\"\n",
    "tensorflow_version = '2.1.0'\n",
    "python_version = 'py3'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b0e21f-6135-4bba-9abe-65cd1b8fb8cc",
   "metadata": {},
   "source": [
    "## Define Trained Models For Inference Pipeline \n",
    "Before we could deploy an inference pipeline, we need to define the models to be included. The first step is to define the model objects and associate them with a SageMaker Pipeline model as shown in following order:\n",
    "\n",
    "1. Define model objects (between 2-15 model objects).\n",
    "2. Create a SageMakerPipeline model object and associate the model objects created in the previous into the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ac1fc3-071d-4f97-8dfb-e7a1c7cd9625",
   "metadata": {},
   "source": [
    "First, let's restore the SageMaker feature store group names that we've created in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08beb1f-4582-48d9-a9c3-7cb55a39d5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2f21fd-fca9-4361-a419-b8f66cec8962",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.pipeline import PipelineModel\n",
    "from time import gmtime, strftime\n",
    "\n",
    "model_envs = {\n",
    "    \"titles_feature_group_name\": titles_feature_group_name,\n",
    "    \"titles_embedding_mapping_group_name\": titles_embeddings_mappings_feature_group_name,\n",
    "    \"AWS_DEFAULT_REGION\": session._region_name\n",
    "}\n",
    "    \n",
    "sklearn_model = SKLearnModel(model_data=knn_model_artifact,\n",
    "                             role=role,\n",
    "                             entry_point=\"custom_inference.py\",\n",
    "                             framework_version=\"1.0-1\",\n",
    "                             sagemaker_session=session,\n",
    "                             source_dir='sklearn/src', \n",
    "                             env = model_envs)\n",
    "\n",
    "tensorflow_model = TensorFlowModel(model_data=s3_model_artifact_tensorflow,\n",
    "                        framework_version=tensorflow_version,\n",
    "                        role=role,\n",
    "                        sagemaker_session=session,\n",
    "                        entry_point=\"inference.py\",\n",
    "                        source_dir=\"ncf/src\")\n",
    "\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "endpoint_name = \"inference-pipeline-ep-\" + timestamp_prefix\n",
    "\n",
    "sm_model = PipelineModel(\n",
    "    name=f\"recsys-inference-pipeline-{timestamp_prefix}\", role=role, models=[sklearn_model, tensorflow_model], sagemaker_session=session)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cf36a5-bb59-4335-8e09-b834fe2d7ecd",
   "metadata": {},
   "source": [
    "Deploy the serial inference pipeline by invoking the model.deploy method and choosing an appropriate instance type. Optionally provide a name of the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7baae0-5252-4e4c-8f73-0388bfc418c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm_model.deploy(initial_instance_count=1, instance_type=\"ml.c5.4xlarge\", endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3866323b-9807-4f00-9804-ebd2a25d7035",
   "metadata": {},
   "source": [
    "# Testing the Serial Inference Pipeline\n",
    "Once the serial inference model is deployed, we can test the functionality to ensure it works as expected. SageMaker SDK provides an easy way to invoke the inference endpoint using a Predictor class, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ba65c8-8072-4d5b-8ff5-9452f45e09fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "import json\n",
    "\n",
    "predictor = Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=session,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6914884a-12dd-44c0-92cc-f968a2801b28",
   "metadata": {},
   "source": [
    "# Test Serial Inference Pipeline With Sample Data\n",
    "Once the serial inference pipeline has been deployed successfully, we want to send some inference requests to the deployed endpoint to ensure the end to end orchestration works as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62161b1-0841-4746-8594-449e4bd85df3",
   "metadata": {},
   "source": [
    "We'll use the given movies dataset in the data folder to perform inferences to validate the deployed endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f526388c-8fcb-45e6-8ad4-0356fccc528a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "movies_df = pd.read_csv(\"data/movies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499ecb27-1c72-4399-bc03-dbb48cac5ac2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8e3f2c-3ade-40df-b6b0-5a43f18b694d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_movie_id = 2139\n",
    "sample_user_id = \"123\"\n",
    "test_data = { \"user_id\" :  sample_user_id, \"movie_id\" : str(sample_movie_id) }\n",
    "payload = json.dumps(test_data)\n",
    "predictions = predictor.predict(payload)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2a8736-d4e6-4338-9772-9d6ccb98d078",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "featurestore_runtime_client = boto3.client('sagemaker-featurestore-runtime')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769b17e9-d687-412f-b21b-1cc8b9a15137",
   "metadata": {},
   "source": [
    "# Transform the Prediction Results\n",
    "After invoking the endpoint successfully, we have a list of item ids returned from the serial inference pipeline. In the next cell, we will transform these movie ids back to the titles for better readability. We use the feature store group that contains the mappings of movie ids and the associated item_ids (index) to retrieve movie_ids. Detail is shown in the following: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c3981f-fccc-44a7-a448-1ec14591692e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "recommended_movies = []\n",
    "for prediction in predictions['predictions']:\n",
    "    item_id = prediction['item_id']\n",
    "    title_embedding_feature_record = featurestore_runtime_client.get_record(FeatureGroupName=titles_embeddings_mappings_feature_group_name, \n",
    "                                                        RecordIdentifierValueAsString=str(item_id))\n",
    "    movie_id = [ x['ValueAsString'] for x in title_embedding_feature_record[ 'Record'] if x['FeatureName'] == 'movieId' ][0]\n",
    "    matching_movie = movies_df[movies_df['movieId'] == int(movie_id)]\n",
    "    recommended_movies.append(matching_movie['title'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffa8ff7-f13a-4b26-9883-c7f95ce81925",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "watched_movie = movies_df[movies_df['movieId'] == int(sample_movie_id)]['title'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f0fade-7ee1-44bc-8e24-4cc3deee284d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"The top 5 recommended movies after watching '{watched_movie}' are as followed:\\n\")\n",
    "for recommended_movie in recommended_movies:\n",
    "    print(recommended_movie)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14df46fc-2e35-44b7-a1da-8ba6c978b8fb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Performance Test\n",
    "Finally, we'll run some performance tests against the serial inference endpoint to measure the total latency. We use the test data given in the lab to perform the test as followed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae71cfce-b815-400a-9b08-ef2569f3b036",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0f0690-b5ed-4ef1-b160-9e3295256574",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_movie_id = 2139\n",
    "sample_user_id = \"123\"\n",
    "test_data = { \"user_id\" :  sample_user_id, \"movie_id\" : str(sample_movie_id) }\n",
    "payload = json.dumps(test_data)\n",
    "pred_time = []\n",
    "for i in range(100):\n",
    "    start = time.time()\n",
    "    predictions = predictor.predict(payload)\n",
    "    pred_time.append(time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc30693-8fbe-4ab4-b5d3-cc4562c0b4e3",
   "metadata": {},
   "source": [
    "Finally, we visualize the p95, p90 and average latency based on the metrics captured from the test above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956eef06-5c6c-4293-9936-c81426f9a425",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\nPredictions time statistic: \\n\")\n",
    "print('\\nP95: ' + str(np.percentile(pred_time, 95)*1000) + ' ms\\n')    \n",
    "print('P90: ' + str(np.percentile(pred_time, 90)*1000) + ' ms\\n')\n",
    "print('Average: ' + str(np.average(pred_time)*1000) + ' ms\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1c5e8d-e388-456a-8b61-1dbef6abc360",
   "metadata": {},
   "source": [
    "## Amazon Cloudwatch Metrics\n",
    "In the following section, we visualize the relevant cloudwatch metrics for the inference pipeline.\n",
    "We will focus on the metrics around invocation counts, server/client errors and model latency.\n",
    "\n",
    "### Cloudwatch Metric namespace: AWS/SageMaker\n",
    "* Metrics Frequency: 1-minute.\n",
    "* Overhead latency: The time that it takes to transport a request to the model container from the SageMaker Runtime Runtime API and transport the response back to the SageMaker Runtime Runtime API.\n",
    "* Model latency: The time that it takes the model container to process the request and return a response.\n",
    "* Container Latency: The time that it takes the container to process the request and return a response. \n",
    "* Container CPU Consumption: The number of CPUs used in processing the incoming request\n",
    "* Container Memory Consumption: The amount of memory used in procesing the incoming request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d260186-9456-4ad3-b86c-b356bff9bc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cw_client = boto3.client(\"cloudwatch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c26421e-7403-4745-997b-b30c81d7785a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_cw_metrics(endpoint_name):\n",
    "    images = []\n",
    "    stat = \"Average\"\n",
    "    # Container/Model Latency\n",
    "    metrics = [\n",
    "        [ \"AWS/SageMaker\", \"ModelLatency\", \"EndpointName\", endpoint_name, \"VariantName\", \"AllTraffic\" ],\n",
    "        [ \"AWS/SageMaker\", \"OverheadLatency\", \"EndpointName\", endpoint_name, \"VariantName\", \"AllTraffic\" ],\n",
    "        [ \".\", \"ContainerLatency\", \".\", \".\", \".\", \".\", \"ContainerName\", \"container-2\" ],\n",
    "        [ \"...\", \"container-1\" ]]\n",
    "            \n",
    "    metric_widget = {\n",
    "        \"metrics\": metrics,\n",
    "        \"view\": \"timeSeries\",\n",
    "        \"stacked\": False,\n",
    "        \"stat\": stat,\n",
    "        \"period\": 60,\n",
    "        \"width\": 1000,\n",
    "        \"height\": 200,\n",
    "    }\n",
    "    response = cw_client.get_metric_widget_image(\n",
    "        MetricWidget=json.dumps(metric_widget)\n",
    "    )\n",
    "    \n",
    "    images.append(Image.open(io.BytesIO(response[\"MetricWidgetImage\"])))\n",
    "\n",
    "    # Container CPU Utilization\n",
    "    metrics = [[ \"/aws/sagemaker/Endpoints\", \"CPUUtilization\", \"EndpointName\", endpoint_name, \n",
    "                \"VariantName\", \"AllTraffic\", \"ContainerName\", \"container_1\" ],\n",
    "                [ \"...\", \"container_2\" ]]\n",
    "\n",
    "    metric_widget = {\n",
    "        \"metrics\": metrics,\n",
    "        \"view\": \"timeSeries\",\n",
    "        \"stacked\": False,\n",
    "        \"stat\": stat,\n",
    "        \"period\": 60,\n",
    "        \"width\": 1000,\n",
    "        \"height\": 200,\n",
    "    }\n",
    "    response = cw_client.get_metric_widget_image(\n",
    "        MetricWidget=json.dumps(metric_widget)\n",
    "    )\n",
    "\n",
    "    images.append(Image.open(io.BytesIO(response[\"MetricWidgetImage\"])))\n",
    "\n",
    "\n",
    "    # Container Memory Utilization\n",
    "    metrics = [\n",
    "            [ \"/aws/sagemaker/Endpoints\", \"MemoryUtilization\", \"EndpointName\", endpoint_name, \n",
    "             \"VariantName\", \"AllTraffic\", \"ContainerName\", \"container_1\" ],\n",
    "            [ \"...\", \"container_2\" ]]\n",
    "\n",
    "    metric_widget = {\n",
    "        \"metrics\": metrics,\n",
    "        \"view\": \"timeSeries\",\n",
    "        \"stacked\": False,\n",
    "        \"stat\": stat,\n",
    "        \"period\": 60,\n",
    "        \"width\": 1000,\n",
    "        \"height\": 200,\n",
    "    }\n",
    "    response = cw_client.get_metric_widget_image(\n",
    "        MetricWidget=json.dumps(metric_widget)\n",
    "    )\n",
    "\n",
    "    images.append(Image.open(io.BytesIO(response[\"MetricWidgetImage\"])))\n",
    "\n",
    "    # Invocattions \n",
    "    metrics = [[ \"AWS/SageMaker\", \"Invocations\", \"EndpointName\", endpoint_name, \"VariantName\", \"AllTraffic\" ],\n",
    "        [ \".\", \"InvocationsPerInstance\", \".\", \".\", \".\", \".\" ]]\n",
    "\n",
    "    metric_widget = {\n",
    "        \"metrics\": metrics,\n",
    "        \"view\": \"timeSeries\",\n",
    "        \"stacked\": False,\n",
    "        \"stat\": stat,\n",
    "        \"period\": 60,\n",
    "        \"width\": 1000,\n",
    "        \"height\": 200,\n",
    "    }\n",
    "    response = cw_client.get_metric_widget_image(\n",
    "        MetricWidget=json.dumps(metric_widget)\n",
    "    )\n",
    "\n",
    "    images.append(Image.open(io.BytesIO(response[\"MetricWidgetImage\"])))\n",
    "    \n",
    "    for image in images:\n",
    "        image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241fe62c-ce31-4754-915f-a4335893395b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_cw_metrics(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c943aa5e-19dd-4291-a862-5d81bf720cb5",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this notebook, we learn how to deploy a SageMaker Serial Inference Pipeline with a scikit-learn KNN, and a Tensorflow model. To validate the latency, we ran performance tests against the deployed endpoint and showed the total latency based on the test data. \n",
    "We also use Cloudwatch metrics to visualize the model latency and compute resource utilization at the container level as well as the instance level. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94707366-e377-4028-b07d-a12e46f81ba6",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e01485-e7de-4698-bb4f-7f13a16a8424",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a538bc23-c71d-49ed-be3f-f2b6962d2c48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
